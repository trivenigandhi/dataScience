{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook runs the mae feature selection function to determine the best features to use in the clustering models. I use the 2015-2016 SQR data to inform feature selection in order to prevent overfitting the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from maeFeatureSelection import maeFeatureSelection\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For this first set of analyses, I chose to drop na values on the target variables rather than impute a mean or median value across missing records. Schools with cohort sizes less than 20 students are not reported on in NYC data, so imputing values for these schools would overestimate the size of the effect between the features and various targets. Additionally, using the correlation heat map from the initial visualizations script, I remove certain features that are highly correlated with one another- such as 8th grade English and Math Proficency, instead opting to only use one of those variables. Additionally the economic need index is a composite score of the percent of students in temp housing and those that are HRA eligible, so I opt to only use that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/sqrAnalysisData.csv\")\n",
    "sy = data[data[\"schoolYear\"]=='2015_2016'].copy()\n",
    "syFeatureNames = ['averageGrade8MathProficiency','percentEnglishLanguageLearners',\n",
    "                  'percentStudentswithDisabilities','percentSelfContained','economicNeedIndex',\n",
    "                  'percentAsian','percentBlack','percentHispanic','percentWhite']\n",
    "naDropSy = sy.dropna(subset = ['4YearGraduationRate','collegeandCareerPreparatoryCourseIndex',\n",
    " '4YearGraduationRateBlackorHispanicMalesinLowestThirdCitywide',\n",
    " '4YearGraduationRateEnglishLanguageLearners'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values='NaN', strategy='mean')\n",
    "syFeatures = imp.fit_transform(naDropSy.loc[:,[\n",
    "    'averageGrade8EnglishProficiency','averageGrade8MathProficiency',\n",
    "    'percentEnglishLanguageLearners','percentStudentswithDisabilities','percentSelfContained',\n",
    "    'economicNeedIndex','percentinTempHousing','percentHRAEligible','percentAsian',\n",
    "    'percentBlack','percentHispanic','percentWhite']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradRate = naDropSy.loc[:,'4YearGraduationRate']\n",
    "ccpci = naDropSy.loc[:,'collegeandCareerPreparatoryCourseIndex']\n",
    "bhLowestThirdGradRate = naDropSy.loc[:, '4YearGraduationRateBlackorHispanicMalesinLowestThirdCitywide']\n",
    "ellGradRate = naDropSy.loc[:, '4YearGraduationRateEnglishLanguageLearners']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradModel = maeFeatureSelection(data=syFeatures,target=gradRate,featureNames=syFeatureNames) \n",
    "ccpciModel = maeFeatureSelection(data=syFeatures,target=ccpci,featureNames=syFeatureNames) \n",
    "bhModel = maeFeatureSelection(data=syFeatures,target=bhLowestThirdGradRate,featureNames=syFeatureNames) \n",
    "ellModel = maeFeatureSelection(data=syFeatures,target=ellGradRate,featureNames=syFeatureNames) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unfortunately these results indicate that the additive models have a neglible effect on improving the mean absolute error. Given the relatively small number of features in the dataset, I move forward with clustering the data on all features. An alternative method would be to use PCA or a Lasso model to reduce dimensionality before clustering. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
