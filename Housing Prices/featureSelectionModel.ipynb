{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "##### In this notebook, I experiment with different feature selection models to par down the number of features in the final model. Before doing so, I convert all categorical data to dummy variables, and standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from numpy.random import RandomState\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/Triveni/Desktop/dataScience/data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "catVars = [col for col in list(train) if train[col].dtype==\"object\"]\n",
    "trainPlusDummies = pd.get_dummies(data=train,columns=catVars) # create dummy variables\n",
    "trainPlusDummies.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainPlusDummies.drop(columns=[\"SalePrice\",\"Id\"])\n",
    "Y = trainPlusDummies.SalePrice\n",
    "featureNames = list(X)\n",
    "x_train,x_test,y_train,y_test = sklearn.model_selection.train_test_split(X,Y)\n",
    "X_scaler = StandardScaler() #create the scaler and apply it to the train and test sets \n",
    "x_train = X_scaler.fit_transform(x_train)\n",
    "x_test = X_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I first try recursive feature elimination using a vanilla linear regression for the estimator. The rfe score is quite low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46405107062861578"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModel = LinearRegression()\n",
    "rfe = RFECV(estimator=linearModel,cv=5)\n",
    "rfe.fit(x_train, y_train)\n",
    "rfe.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then I try the same feature elimination using a Lasso model. I set the random state ahead, so that my results can be reproduced. The results are better than the previous model but not enough to instill confidence in the feature selection fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61359545327804965"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(random_state=np.random.RandomState(5))\n",
    "rfe = RFECV(estimator=lasso,cv=5)\n",
    "rfe.fit(x_train, y_train)\n",
    "rfe.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, I use a Random Forest Regressor to find optimal features. I set the max number of features to the square root of the total number of features to reasonably constrain the data. This model provides the best score of all the models so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81588047216909565"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.random.RandomState(3)\n",
    "rf = RandomForestRegressor(random_state = 3,max_features='sqrt')\n",
    "rf.fit(x_train,y_train)\n",
    "rf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For reference, the 'rfAll' variable shows an almost equal score for a Random Forest Regressor using all features in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82775069916268917"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.random.RandomState(3)\n",
    "rfAll = RandomForestRegressor(random_state = 3)\n",
    "rfAll.fit(x_train,y_train)\n",
    "rfAll.score(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
